
\section{Scanner and Parser for Contexts}

This file contains the specifications for a scanner (unsing OCamlLex)
and a parser (unsing OCamlYacc) for context table. A contex table
associates an object with attributes, like in the following two lines:

\begin{quote}
\begin{verbatim}
chmod:		change file mode permission ;
chown:		change file group owner;
\end{verbatim}
\end{quote}

An object is separeted from its attributes by a colon. Attributes are
separated by whitespace and end with a semicolon.

% ------------------------------------------------------------------ 
\subsection{Parser}
% ------------------------------------------------------------------ 

<<parser.mly>>=
%{
    exception Error of string
    let error fmt = Printf.kprintf (fun msg -> raise (Error msg)) fmt
    let (@@) f x  = f x
%}
<<types>>
%%
<<grammar>>
@

A context is represented as a list of pairs. Each pair represents on
object and its associated attributes. Everything is represented as a
string.

Idea: to increase sharing we should build a cache for strings.

<<types>>=
%start context
%type <(string * string list) list> context

%token EOF SEMI COLON
%token <string> ID
@

Rules are simple; both the list of objects and attributes may be simple.
We use left recursion because this is efficient when doing bottom-up
(Yacc) parsing.

<<grammar>>=
context             : objects EOF                   { $1        }

objects             : objects obj                   { $2 :: $1  }
                    | /**/                          { []        }

obj                 : ID COLON attributes SEMI      
                      { Debug.progress 'o' @@ ($1 , $3)   }
                    
attributes          : attributes ID                 { $2 :: $1  }
                    | /**/                          { []        }
@

% ------------------------------------------------------------------ 
\subsection{Scanner}
% ------------------------------------------------------------------ 

The scanner splits the input into tokens. Tokens are defined by the
parser (in [[parser.mli]]).

<<scanner.mli>>=
val token:          Lexing.lexbuf -> Parser.token  

<<scanner.mll>>=
{

    module P = Parser   (* tokens are defined here *)

    let get         = Lexing.lexeme
    let getchar     = Lexing.lexeme_char
    let strlen      = String.length
    let pos_start   = Lexing.lexeme_start
    let pos_end     = Lexing.lexeme_end

    let atoms       = Atom.create ()
    let atomize str = Atom.atomize atoms str
    
    exception Error of string
    let error fmt   = Printf.kprintf (fun msg -> raise (Error msg)) fmt
    let (@@) f x    = f x

}
@

The declarations below define regular expressions that we use to define
the regular expressions for tokens. An identifier can be almost anything
as long as it does not include a colon, semicolon, or whitespace. 

<<scanner.mll>>=
let digit           = ['0'-'9']
let letter          = ['a'-'z' 'A'-'Z']
let misc            = ['!'-'/' '<''=''>''?''@' '[''\\'']''^''_''`''{''|''}''~']
let special         = [';'':']
let alpha           = (letter|digit|misc)
let id              =  alpha alpha* 
                     | alpha (alpha|special)* alpha

let ws              = [' ' '\t' '\r']
let nl              = '\n'
@

A comment is introduced by either [[%]], [[--]], or [[#]] and reaches up
to the end of line, not including the newline. The last rule catches any
input not matched by any other rule. Thus, we use it to detect illegal
input.

<<scanner.mll>>=
rule token = parse
    eof             { P.EOF }
  | ws+             { token lexbuf }
  | nl              { token lexbuf }

  | '%' [^'\n']*    { token lexbuf }
  | "--"[^'\n']*    { token lexbuf } 
  | "#" [^'\n']*    { token lexbuf } 

  | id              { P.ID (atomize @@ get lexbuf)} 
  | ';'             { P.SEMI            }
  | ':'             { P.COLON           }

  | _               { let str    = String.escaped @@ get lexbuf in
                      let off    = Lexing.lexeme_start lexbuf in
                        error "illegal character: '%s' at offset %d" str off
                    }  
@

